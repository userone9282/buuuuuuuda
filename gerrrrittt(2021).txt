# 1

#import data
#rename dataset
DATASET <- read.csv("DATASET.csv - DATASET.csv.csv", stringsAsFactors = TRUE)

 
# 1i)

# drop client id
DATASET <- DATASET[-1]


# other factors
DATASET$Attrition_Flag <- factor(DATASET$Attrition_Flag)
DATASET$Dependent_count <- factor(DATASET$Dependent_count)

DATASET_norm <- DATASET[,unlist(lapply(DATASET, is.numeric))]
DATASET_factors <- DATASET[,unlist(lapply(DATASET, is.factor))]

str(DATASET_norm)
str(DATASET_factors)

# standard normalize function
## call z-score normalization
normalize <- function(x){
  (x - mean(x)) / sqrt(var(x)) 
}
# apply normalization
DATASET_norm <- as.data.frame(lapply(DATASET_norm, normalize))

#bind normalized and factors DATASET
DATASET <- cbind(DATASET_factors,DATASET_norm)

# ----------------ii ---------------
# stratified random sample
set.seed(123)
DATASET_rand <-DATASET[order(runif(10127)),]

# spliting into 70:30
0.7*10127
## = 7088.9
## = 7089

DATASET_train<-DATASET_rand[(1:7089),]
DATASET_test<-DATASET_rand[(7090:10127),]

# create features and labels

## -----------iii----------------------
#implementing C5O
library(C50)
model1 <- C5.0(DATASET_train[-1],DATASET_train$Attrition_Flag)
summary(model1)
model1_pred<-predict(model1,DATASET_test[-1])
CrossTable(DATASET_test$Attrition_Flag,  model1_boost_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual', 'predicted '))

# Evaluation on training data (7089 cases):
  
# Decision Tree   
# ----------------  
#   Size      Errors  
# 
# 99  160( 2.3%)   <<
#   
#   
#   (a)   (b)    <-classified as
# ----  ----
#   5899    54    (a): class 0
# 106  1030    (b): class 1
# 
# 
# Attribute usage:
#   
#   100.00%	Total_Trans_Ct
# 94.85%	Total_Trans_Amt
# 47.13%	Total_Revolving_Bal
# 43.81%	Total_Relationship_Count
# 34.93%	Total_Ct_Chng_Q4_Q1
# 15.49%	Months_Inactive_12_mon
# 15.32%	Avg_Utilization_Ratio
# 10.79%	Customer_Age
# 7.43%	Marital_Status
# 7.43%	Total_Amt_Chng_Q4_Q1
# 6.98%	Gender
# 3.55%	Months_on_book
# 2.58%	Avg_Open_To_Buy
# 2.47%	Contacts_Count_12_mon
# 1.11%	Dependent_count
# 0.34%	Education_Level
# 
# 
# Time: 0.2 secs

  # | predicted  
  # actual |         0 |         1 | Row Total | 
  #   -------------|-----------|-----------|-----------|
  #   0 |      2514 |        33 |      2547 | 
  #   |     0.828 |     0.011 |           | 
  #   -------------|-----------|-----------|-----------|
  #   1 |        75 |       416 |       491 | 
  #   |     0.025 |     0.137 |           | 
  #   -------------|-----------|-----------|-----------|
  #   Column Total |      2589 |       449 |      3038 | 
  #   -------------|-----------|-----------|-----------|
  

##-------------------vi------------------------------

##Boosting the accuracy of decision trees using adaptive boosting

model1_boost <- C5.0(DATASET_train[-1],DATASET_train$Attrition_Flag,
                     trials = 10)
model1_boost_pred<-predict(model1_boost,DATASET_test[-1])
CrossTable(DATASET_test$Attrition_Flag,  model1_boost_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual', 'predicted '))


# | predicted  
# actual |         0 |         1 | Row Total | 
#   -------------|-----------|-----------|-----------|
#   0 |      2514 |        33 |      2547 | 
#   |     0.828 |     0.011 |           | 
#   -------------|-----------|-----------|-----------|
#   1 |        75 |       416 |       491 | 
#   |     0.025 |     0.137 |           | 
#   -------------|-----------|-----------|-----------|
#   Column Total |      2589 |       449 |      3038 | 
#   -------------|-----------|-----------|-----------|


## ------------------------v--------------------------
#Model performance.
library(caret)
library(gmodels)
model1_pred<-predict(model1,DATASET_test[-1])
confusionMatrix(DATASET_test$Attrition_Flag,  model1_pred,
                dnn = c('actual ', 'predicted '))

# Confusion Matrix and Statistics
# 
# predicted 
# actual     0    1
# 0 2478   69
# 1   95  396
# 
# Accuracy : 0.946           
# 95% CI : (0.9374, 0.9538)
# No Information Rate : 0.8469          
# P-Value [Acc > NIR] : < 2e-16         
# 
# Kappa : 0.7964          
# 
# Mcnemar's Test P-Value : 0.05092         
#                                           
#             Sensitivity : 0.9631          
#             Specificity : 0.8516          
#          Pos Pred Value : 0.9729          
#          Neg Pred Value : 0.8065          
#              Prevalence : 0.8469          
#          Detection Rate : 0.8157          
#    Detection Prevalence : 0.8384          
#       Balanced Accuracy : 0.9073          
#                                           
#        'Positive' Class : 0 

#vi) 10 fold cv
# knn with 10 fold cross validation
# define control parameter
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)# tune control

model2 <- train(DATASET_train$Attrition_Flag ~ ., data = DATASET_train, method = "C5.0",
                 trControl=trctrl, tuneLength = 10)
# predictions
model2_pred <- predict(object=model2,DATASET_test[,-1])

confusionMatrix(DATASET_test$Attrition_Flag,  model1_pred,
                dnn = c('actual ', 'predicted '))
                

#2
credit <- read.csv("DATASET.csv",stringsAsFactors = TRUE) # Loading and importing the dataset
str(credit)

credit <- credit[-1]#Dropping the ICLIENTNUM feature - it's a unique Identification
credit$Attrition_Flag <- factor(credit$Attrition_Flag,levels = c("0","1")) # making attrition a factor

attach(credit)

## histograms
hist(Customer_Age)
hist(Card_Category)
hist(Total_Relationship_Count)
hist(Months_on_book)
## box plots
boxplot(Customer_Age)
boxplot(Card_Category)
boxplot(Total_Relationship_Count)
boxplot(Months_on_book)

normalize <- function(x) {
  return((x-min(x))/(max(x)-min(x)))
} # creating a normalize function
credit_n <- as.data.frame(lapply(credit[c(-4,-6,-7,-8,-9)], normalize)) # normalizing the data



4
#Question 4
Data <- read.csv("DATA.csv", stringsAsFactors = T)
str(Data)
#i
library(psych)
pairs.panels(Data[c("price","mpg","mileage","year")])
#price and mpg have -0.6 correlation with a loess smooth sloping downwards and flattens toward the end meaning price goes down with more mpg up to a point.
#price and mileage have -0.54 correlation with a loes smooth slope downwards and flattens towards the end meaning price goes down with more mileage up to a point.
#price and year have a 0.59 correlation with a loess smooth sloping upwards and flattens toward the end showing price increases with the year up to a point.
#mpg and mileage have a 0.4 correlation with a loess smooth slo[ing upwards meaning mpg increases with mileage.
#mpg and year have -0.35 correlation with a downward sloping loess smooth showing mpg reduces with increase in the year.
#mileage and year have -0.79 correlation with a sharply downward sloping loess smooth showing a large decrease in mileage with rhe increase in the year.

#ii
Data_model<- lm(price~ ., data = Data[-1]) 
summary(Data_model)
#year, fuelTypeHybrid and engineSize have a positive coefficient meaning they increase price as they increase.
#transmissionManual, transmissionSemi-Auto, mileage, fuelTypePetrol,tax and mpg have negative coefficients meaning they reduce price as they increase.
#The model has an R-squared of 0.8016 meaning that 80% of the price is explained by our model.

#iii
#mpg and mileage have an effect only once mean price is reached
#Interaction between mpg & engineSize and year & mileage
Data$mpg2 <- ifelse(Data$price >= mean(Data$price), Data$mpg*1, 0)
Data$mileage2 <- ifelse(Data$price >= mean(Data$price), Data$mileage*1, 0)
price~ mpg*engineSize
price~ year*mileage

Data_model2 <- lm(price~ year+transmission+mileage+fuelType+tax+mpg+engineSize+mpg2+mileage2+mpg*engineSize+year*mileage, data = Data)
summary(Data_model2)
#year,transmissionSemi-Auto, mileage, fuelTypeHybrid, mpg, engineSize and mpg2 have a positive coefficient meaning they increase the price.
#transmissionManual, fueltypePetrol, tax, mileage2, mpg:engineSize and year:mileage have negative coefficients meaning they reduce the price.
#The model has an R-squared of 0.8492 meaning that 85% of the price is explained by the model.
#The consultant's advice increased the performance of the model therefore it was valuable.
